general:
  debug: false

train:
  mixup:
    enable: false

callback:
  lr_monitor:
    enable: true
  early_stopping:
    enable: false
  model_loss_checkpoint:
    enable: true
  model_score_checkpoint:
    enable: true

logger:
  csv_logger:
    enable: true

  wandb_logger:
    enable: true
    group: exp_000

trainer:
  accumulate_grad_batches: 1
  precision: 16
  max_epochs: 20 # schduler
  resume_from_checkpoint: None
  stochastic_weight_avg: false


dataset:
  store_train: true
  store_valid: true
  grayscale: false
  gradcam: false
  loader:
    batch_size: 32
    num_workers: 8

transforms:
  train_version: v1
  valid_version: v1
  params:
    height: 224
    width: 224
    p: 0.5

model:
  name: net
  params:
    base_model: swin_base_patch4_window7_224
    pretrained: true
    checkpoint_path:
    num_classes: 2
    neck_version:
    head_version: v1

optimizer:
  name: AdamW

scheduler:
  name: CosineAnnealingWarmRestarts
  interval: step # [step, epoch]
  monitor: val_loss # [val_loss, val_score]

loss:
  names: ["CrossEntropyLoss"]
  weights: [1]

metric:
  names: ["AUROC", "CohenKappa"]
