defaults:
  - _self_
  - callback: base_callback_params
  - optimizer: base_optimizer_params
  - scheduler: base_scheduler_params
  - loss: base_loss_params
  - metric: base_metric_params

general:
  debug: false
  seed: 42
  exp_dir: ./outputs

train:
  mixup:
    enable: true
    alpha: 1
    duration: 0
    p: 0.5

logger:
  csv_logger:
    enable: true
    save_dir: logs
    name: csv_logger
    version: fold

  wandb_logger:
    enable: True
    entity: ishtos
    save_dir: .
    name: fold
    offline: false
    project: {{ cookiecutter.project }}
    log_model: all # ['all', true, false]
    group: exp_000

trainer:
  accumulate_grad_batches: 1
  amp_backend: native
  benchmark: false
  deteministic: true
  gpus: 1
  gradient_clip_val: 0
  gradient_clip_algorithm: norm
  precision: 16
  max_epochs: 20 # schduler
  resume_from_checkpoint: None
  stochastic_weight_avg: false

preprocess:
  base_dir: ../../data
  image_dir: train/image
  test_image_dir: test/image
  base_csv: csv/base.csv
  fold:
    name: StratifiedKFold
    n_splits: 5
    group: # For GroupKFold

dataset:
  train_df: train.csv
  test_df: test.csv
  id: id
  target: label
  fold:
    name: StratifiedKFold
    n_splits: 5
    group: # For GroupKFold
  store_train: true
  store_valid: true
  grayscale: false
  gradcam: false
  loader:
    batch_size: 32
    num_workers: 8

transforms:
  train_version: v1
  valid_version: v1
  params:
    height: 224
    width: 224
    p: 0.5

model:
  name: swin
  params:
    base_model: swin_base_patch4_window7_224
    pretrained: true
    checkpoint_path:
    num_classes: 2
    neck_version:
    head_version: v1

optimizer:
  name: AdamW

scheduler:
  name: CosineAnnealingWarmRestarts
  interval: epoch # [step, epoch]
  monitor: val_loss # [val_loss, val_score]

loss:
  names: ["CrossEntropyLoss"]
  weights: [1]

metric:
  name: ["AUROC"]
